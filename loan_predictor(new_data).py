# -*- coding: utf-8 -*-
"""Loan_Predictor(new data).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17VdX-vKCoPlQgKgsFDL-Hx2KdVYq4Edf

# **Loading the data**
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from google.colab import files
from datetime import datetime
import seaborn as sns
# %matplotlib inline

import io

# Load the data
local_file = files.upload()
train_data = io.BytesIO(local_file['train1.csv'])
train_data2 = io.BytesIO(local_file['train.csv'])
df1 = pd.read_csv(train_data)
df2 = pd.read_csv(train_data2)

"""# **Data integration**"""

df1.info()

df2.info()

"""Creating one dataframe out of the two dataframes from the csv files."""

frames = [df1, df2]

df = pd.concat(frames)
df

df.tail()

"""# **Data analysis:**

Checking the dimension of the dataset and the features.
"""

df.describe()

"""
Here we can see the shape of our data with the ``` .shape ```. Here we see (981, 13) this means that we have  a 981 rows and 13 columns"""

df.shape

"""Here we can see the shape of our test data with the .shape. Here we see (367, 12) this means that we have a 367 rows and 12 columns

To view what data that is stored we can use .columns. This will return the colums of our data
"""

df.columns

"""To look at the data we'll use the .head() method from pandas. This will show us the first 5 items in our dataframe."""

#First 5 rows of our dataset
df.head()

#Last 5 rows of our dataset
df.tail()

df.info()

""">It can be seen that there are features that are numeric and also objects. Later, the ones that are not numeric will have to be converted into either float or int in order to be plotted and then used for the trainig of the models. There are also missing values in the dataset, which will be handled later."""

# Find columns with missing values and their percent missing
df.isnull().sum()                                                     
miss_val = df.isnull().sum().sort_values(ascending=False)
miss_val = pd.DataFrame(data=df.isnull().sum().sort_values(ascending=False), columns=['MissvalCount'])

# Add a new column to the dataframe and fill it with the percentage of missing values
miss_val['Percent'] = miss_val.MissvalCount.apply(lambda x : '{:.2f}'.format(float(x)/df.shape[0] * 100)) 
miss_val = miss_val[miss_val.MissvalCount > 0].style.background_gradient(cmap='Reds')
miss_val

""">The light red color shows the small amount of NaN values. If the features were with a high than 50% of missing values, they would have to be removed. Yet, in this case, they have relatively low percentage so they can be used in future. Then, the NaN values will be replaced.

Sorting the data by `Loan_Status`and showing 50 elements.
"""

df.sort_values('Loan_Status', ascending = True)[:50]

"""Sorting the data by `Education` and showing 50 elements."""

df.sort_values('Education')[:50]

"""Here we can see one row (one person)"""

df.iloc[0]

"""Get the unique values and their frequency of variable.
(Checking how many times the certain value occurs.)
"""

df['Loan_Status'].value_counts()

df['ApplicantIncome'].value_counts()

df['Gender'].value_counts()

df['Married'].value_counts()

df['CoapplicantIncome'].value_counts()

df['Dependents'].value_counts()

df['Education'].value_counts()

df['Self_Employed'].value_counts()

df['Loan_Status'].unique()

df['ApplicantIncome'].unique()

sns.countplot(y = 'Gender', hue = 'Loan_Status', data = df)

""">The diagram shows on one hand that there are more male applicants than female and on other hand, there are more approved loans than disapproved."""

df['Gender'].value_counts().plot(kind='pie', autopct='%1.2f%%', figsize=(6, 6))

""">The percentage of males who applied for a loan is greater than the one of females."""

df['Loan_Status'].value_counts().plot(kind='pie', autopct='%1.2f%%', figsize=(6, 6))

"""> According to the pie chart, there are more approved loans that
disapproved.
"""

grid=sns.FacetGrid(df, row='Gender', col='Married', size=2.2, aspect=1.6)
grid.map(plt.hist, 'ApplicantIncome', alpha=.5, bins=10)
grid.add_legend()

"""> Males have the highest income according to the data. Males that are married have greater income than unmarried male. And the same goes for females.

"""

grid=sns.FacetGrid(df, row='Gender', col='Education', size=2.2, aspect=1.6)
grid.map(plt.hist, 'ApplicantIncome', alpha=.5, bins=10)
grid.add_legend()

"""> A graduate who is a male has more income than a one whithout and the same goes for females.

Here I am exploring the distribution of the numerical variables mainly the Applicant income and the Loan amount. 

What can be noticed are quite a few outliers.
"""

sns.distplot(df.ApplicantIncome,kde=False)

""">People with better education should normally have a higher income, we can
check that by plotting the education level against the income.
"""

df.boxplot(column='ApplicantIncome', by = 'Education')

"""

> We can conclude that there is no substantial different between the mean income of graduate and non-graduates. However, there are a higher number of graduates with very high incomes, which are appearing to be the outliers.
"""

sns.boxplot(x='Education',y='ApplicantIncome',data=df)

""">The distributions shows that the graduates have more outliers which means that the people with huge income are most likely to be educated."""

temp3 = pd.crosstab(df['Credit_History'], df['Loan_Status'])
temp3.plot(kind='bar', stacked=True, color=['blue','green'], grid=False)

""">This shows that the chances of getting a loan are higher if the applicant has a valid credit history.

# **Data cleaning**

Checking if there are any null values and if so, which.
"""

df.isnull().sum()

"""Converting the string values to numeric valuesto use them in the training of the models.

One-Hot Encoding: 
This process takes categorical variables and converts them to a numerical representation without an arbitrary ordering. What computers know is numbers and for machine learning it is vital to accommodate the feautures into numeric values.
"""

numeric_gender = {'Female': 1, 'Male': 2}
df ['Gender'] = df['Gender'].map(numeric_gender)
numeric_married = {'Yes': 1, 'No': 2}
df ['Married'] = df['Married'].map(numeric_married)
numeric_edu = {'Graduate': 1, 'Not Graduate': 2}
df ['Education'] = df['Education'].map(numeric_edu)
numeric_self = {'Yes': 1, 'No': 2}
df ['Self_Employed'] = df['Self_Employed'].map(numeric_self)
numeric_loan = {'Y': 1, 'N': 2}
df ['Loan_Status'] = df['Loan_Status'].map(numeric_loan)
numeric_property = {'Rural': 1, 'Urban': 2, 'Semiurban': 3}
df ['Property_Area'] = df['Property_Area'].map(numeric_property)
numeric_d = {'3+': 3}
df ['Dependents'] = df['Dependents'].map(numeric_d)

"""Filling up the null values in order to train the model. """

df.fillna(0)

"""# **Data processing:**

Checking if there are certain missing values that need to be fixed.
"""

total = df.isnull().sum().sort_values(ascending=False)
percent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)
missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])
missing_data.head(20)

"""Filling the missing values, for categorical we can fill them with the mode (the value with the highest frequency). The best practice is to use mode with data points such as salary field or any other kind of money."""

df['Gender'] = df['Gender'].fillna(
df['Gender'].dropna().mode().values[0] )
df['Married'] = df['Married'].fillna(
df['Married'].dropna().mode().values[0] )
df['Dependents'] = df['Dependents'].fillna(
df['Dependents'].dropna().mode().values[0] )
df['Self_Employed'] = df['Self_Employed'].fillna(
df['Self_Employed'].dropna().mode().values[0] )
df['LoanAmount'] = df['LoanAmount'].fillna(
df['LoanAmount'].dropna().median() )
df['Loan_Amount_Term'] = df['Loan_Amount_Term'].fillna(
df['Loan_Amount_Term'].dropna().mode().values[0] )
df['Credit_History'] = df['Credit_History'].fillna(
df['Credit_History'].dropna().mode().values[0] )
df['Loan_Status'] = df['Loan_Status'].fillna(
df['Loan_Status'].dropna().mode().values[0] )

"""Checking if there any empty values."""

df.isnull().sum()

"""Some people might have a low income, but strong CoappliantIncome, so a good idea would be to combine them in a TotalIncome column."""

df['LoanAmount_log']=np.log(df['LoanAmount'])
df['TotalIncome']= df['ApplicantIncome'] + df['CoapplicantIncome'] 
df['TotalIncome_log']=np.log(df['TotalIncome'])

sns.distplot(df.TotalIncome,kde=False)

"""# **Modeling**:

Encoding to numeric data in order to start the training of the models.
"""

#drop the uniques loan id
df.drop('Loan_ID', axis = 1, inplace = True)

df['Gender'].value_counts()

df['Dependents'].value_counts()

df.info()

"""Need to covnvert the object values to numeric ones - `Dependents` needs to become an int.

Heatmaps are very useful to find relations between two variables in a dataset and this way the user gets a visualisation of the numeric data. No correlations are extremely high. Each square shows the correlation between the variables on each axis. 

*   The correlations between the feautures can be explained:


> The close to 1 the correlation is the more positively correlated they are; that is as one increases so does the other and the closer to 1 the stronger this relationship is. It is noticable that the correlation between the `ApplicantIncome` and `LoanAmount` is 0.57, which mean that they have a positive correlation, but not strong.
"""

# Commented out IPython magic to ensure Python compatibility.
from pandas import DataFrame
# %matplotlib inline
plt.figure(figsize=(12, 8))
df_temp = df.copy()
Index= ['Gender',	'Married',	'Dependents',	'Education',	'Self_Employed',	'ApplicantIncome',	'CoapplicantIncome',	'LoanAmount',	'Loan_Amount_Term',	'Credit_History',	'Property_Area',	'Loan_Status']
Cols = ['Gender',	'Married',	'Dependents',	'Education',	'Self_Employed',	'ApplicantIncome',	'CoapplicantIncome',	'LoanAmount',	'Loan_Amount_Term',	'Credit_History',	'Property_Area',	'Loan_Status']
df_temp = DataFrame(abs(np.random.randn(12, 12)), index=Index, columns=Cols)

sns.heatmap(df_temp.corr(), annot=True, cmap = 'magma')
plt.show()

"""Importing sklearn libraries"""

from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import f1_score

"""Splitting into train and test set after choosing the right features X and labels y"""

y = df['Loan_Status']
X = df.drop('Loan_Status', axis = 1)

"""To split the dataset, I will use random sampling with 80/20 train-test split; that is, 80% of the dataset will be used for training and set aside 20% for testing:"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)

"""Analyzing the numeric features.

"""

numeric_features = df.select_dtypes(include=[np.number])

numeric_features.columns

# use only those input features with numeric data type 
df = df.select_dtypes(include=["int64","float64"])

# set the target and predictors
y = df.Loan_Status  # target

# use only those input features with numeric data type 
df_temp = df.select_dtypes(include=["int64","float64"]) 

X = df_temp.drop(["Loan_Status"],axis=1)  # predictors

"""Three models will be built and evaluated by their performances with R-squared metric. Additionally, insights on the features that are strong predictors of house prices, will be analised .

**Logistic Regression**
"""

model = LogisticRegression()
model.fit(X_train, y_train)
y_reg=model.predict(X_test)
evaluation = f1_score(y_test, y_reg)
evaluation

"""Reporting the coefficient value for each feature. Notice that the coefficients are both positive and negative. The positive scores indicate a feature that predicts class 1, whereas the negative scores indicate a feature that predicts class 0.

>The importance of a feature is measured by calculating the increase in the model's prediction error after permuting the feature. A feature is "important" if shuffling its values increases the model error, because in this case the model relied on the feature for the prediction.
"""

# get importance
importance = model.coef_[0]
# summarize feature importance
for i,v in enumerate(importance):
	print('Feature: %0d, Score: %.5f' % (i,v))
# plot feature importance
plt.bar([x for x in range(len(importance))], importance)
plt.show()

""">This might mean that your model is underfit (not enough iteration and it has not used the feature enough) or that the feature is not good and you can try removing it to improve final quality.

**Decision tree:**


1.  Creating classifier
2.  Fitting classifier with train data
"""

tree = DecisionTreeClassifier()
tree.fit(X_train, y_train)

"""Do predictions on a test set. **Testing** the model by testing the test data."""

y_tree=tree.predict(X_test)
print(y_tree)

"""Evaluate classsifier, measure accuracy, which is 0.76"""

evaluation = f1_score(y_test, y_tree)
evaluation

""" **Random forests**"""

forest = RandomForestClassifier()
forest.fit(X_train, y_train)

"""**Testing** the model by testing the test data."""

y_forest=forest.predict(X_test)
print(y_forest)

"""Result of the **accuracy**."""

evaluation_f= f1_score(y_test, y_forest)
evaluation_f

"""# Conclusion
From the Exploratory Data Analysis, it can be concluded:

1. The amount of male applicants seems to be greater than the female ones and they tend to live in the semi suburban areas.
1. There are more positive than negative loan statuses - more approvals.
2. The distributions show that the graduates have more outliers which means that the people with huge income are most likely to be educated.
2. Males have the highest income according to the data. Males that are married have greater income that unmarried male. And the same goes for female. Therefore, there is a greater chance for educated and married people to receive a loan than applicant who are not.

From the Modelling, it can be concluded:

1. After the exploring of different types of modelling, that the more accurate model is Logistic Regression than Decision tree.
2. From the evaluation of the three models, it can be noticed that the Random forest performed better than others



"""